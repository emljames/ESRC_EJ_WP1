---
title: "LPA with simulated reading data"
output: 
  html_document:
    toc: true
    toc_float: true
---

This script simulates data designed around the variables available in ALSPAC for the first analysis (Simple View), and uses Mplus to fit a series of latent class models. 

```{r libraries, message = FALSE, warning = FALSE}
library(SIN)
library(MASS)
library(psych)
library(naniar)
library(tidyverse)
library(tidyLPA)
library(GGally)
library(MplusAutomation)
library(texreg)
library(semPlot)
```

```{r set-up}
options(scipen = 999)
set.seed(20201105)
```

# Simulate data

Create dataset similar to one that we will use - three reading accuracy measures, a reading comprehension measure and listening comprehension.  

### Specify variables and relationships

For now, have just used standard score estimates for each task, and made up the correlations.

*TO-DO: Develop into more theoretically meaningful groups. 4 groups, simple view guided.* 

```{r measures}

#CLASS 1

################# MEASURES ###################

# NARA - comprehension
nara_comp_m1 <- 105
nara_comp_sd1 <- 5
  
# NARA - accuracy
nara_acc_m1 <- 105
nara_acc_sd1 <- 5

# Word/nonword reading
word_acc_m1 <- 105
word_acc_sd1 <- 5
nonword_acc_m1 <- 105
nonword_acc_sd1 <-5

# WOLD listening
wold_comp_m1 <- 105
wold_comp_sd1 <- 5

#CLASS 2

################# MEASURES ###################

# NARA - comprehension
nara_comp_m2 <- 80
nara_comp_sd2 <- 7
  
# NARA - accuracy
nara_acc_m2 <- 80
nara_acc_sd2 <- 7

# Word/nonword reading
word_acc_m2 <- 80
word_acc_sd2 <- 7
nonword_acc_m2 <- 80
nonword_acc_sd2 <-7

# WOLD listening
wold_comp_m2 <- 80
wold_comp_sd2 <- 7

#CLASS 3

################# MEASURES ###################

# NARA - comprehension
nara_comp_m3 <- 90
nara_comp_sd3 <- 10
  
# NARA - accuracy
nara_acc_m3 <- 90
nara_acc_sd3 <- 10

# Word/nonword reading
word_acc_m3 <- 90
word_acc_sd3 <- 10
nonword_acc_m3 <- 90
nonword_acc_sd3 <-10

# WOLD listening
wold_comp_m3 <- 90
wold_comp_sd3 <- 10


################# CORRELATIONS ###################

#assumption is that we have same correlation among measures for each class

nara_comp_acc <- 0.5
nara_comp_word <- 0.4
nara_comp_nonword <- 0.4
nara_comp_wold <- 0.6
nara_acc_word <- 0.6
nara_acc_nonword <- 0.5
nara_acc_wold <- 0.2
word_acc_nonword <- 0.7
word_acc_wold <- 0.2
nonword_acc_wold <- 0.2
```

*Decision required: accounting for age differences across tests*
- Option 1: Use test-standardised scores where available, and include age as covariate where not 
- Option 2: Restandardise based on samples 

### Simulate dataset

Compute covariance matrix based on the above estimates, and use to simulate specified number of observations.

```{r simulate-data}
################# COVARIANCES ###################

# List names in order
test_names <- c("nara_comp", "nara_acc", "word_acc", "nonword_acc", "wold_comp")

# List SDs in order above
stddev1 <- c(nara_comp_sd1, nara_acc_sd1, word_acc_sd1, nonword_acc_sd1, wold_comp_sd1)
stddev2 <- c(nara_comp_sd2, nara_acc_sd2, word_acc_sd2, nonword_acc_sd2, wold_comp_sd2)
stddev3 <- c(nara_comp_sd3, nara_acc_sd3, word_acc_sd3, nonword_acc_sd3, wold_comp_sd3)

# Create correlation matrix using above estimates
corr <- matrix(c(1, nara_comp_acc, nara_comp_word, nara_comp_nonword, nara_comp_wold,
                 nara_comp_acc, 1, nara_acc_word, nara_acc_nonword, nara_acc_wold,
                 nara_comp_word, nara_acc_word, 1, word_acc_nonword, word_acc_wold,
                 nara_comp_nonword, nara_acc_nonword, word_acc_nonword, 1, nonword_acc_wold,
                 nara_comp_wold, nara_acc_wold, word_acc_wold, nonword_acc_wold, 1),
               byrow = TRUE, nrow = 5, 
               dimnames = list(test_names, test_names))

# Compute covariance matrix
covar1 <- sdcor2cov(stddev1, corr)
covar2 <- sdcor2cov(stddev2, corr)
covar3 <- sdcor2cov(stddev3, corr)

################# SIMULATE DATA ###################

# List means in order above
means1 <- c(nara_comp_m1, nara_acc_m1, word_acc_m1, nonword_acc_m1, wold_comp_m1)
means2 <- c(nara_comp_m2, nara_acc_m2, word_acc_m2, nonword_acc_m2, wold_comp_m2)
means3 <- c(nara_comp_m3, nara_acc_m3, word_acc_m3, nonword_acc_m3, wold_comp_m3)

# Number of observations
n_ppts <- 1000

#latent profile probabilities.
class_probs<-c(0.6,0.2,0.2)

# generate mixing profile indicator based on profile probabilities
k<-sample(1:3,n_ppts,class_probs,replace=TRUE)

# Set up empty holding dataframe
sim_dat <-data.frame(nara_comp=numeric(), nara_acc=numeric(), word_acc=numeric(), nonword_acc=numeric(), wold_comp=numeric())

#sample from the three multivariate normal distributions in the correct proportions according to profile probabilities.
for(i in 1:n_ppts){
sim_dat[i,]<-switch(k[i],mvrnorm(n = 1, mu = means1, Sigma = covar1), mvrnorm(n = 1, mu = means2, Sigma = covar2), mvrnorm(n = 1, mu = means3, Sigma = covar3))
}
sim_dat <- as.data.frame(cbind(sim_dat,as.factor(k)))

#create a dummy id (change as needed)
sim_dat$id<-paste0("ALSPAC",sprintf('%0.4d', 1:n_ppts))

names(sim_dat)<-c("naraComp", "naraAcc", "wordAcc", "nonwordAcc", "woldComp","k","id")
```

### Missing data

PT: There are different ways to do this and we need to decide which is most realistic for our data. We can remove a percentage of the data completely at random, so random observations are missing, or potentially more likely is that certain individuals will have multiple observations missing, i.e. if there is a missing observation for nara_comp, then I guess its more likely that nara_acc will also be missing too? I've implemented the first option here, but you could add a second step to search for NAs in a row and add extra NAs to that row.

https://stackoverflow.com/questions/50528719/simulate-data-and-randomly-add-missing-values-to-dataframe

**Missing data expectations for this analysis:**
* nara_comp should have no missing data, as inclusion in this/subsequent projects relies on having a comprehension assessment (*unless this should be addressed differently? not quite sure where sample stops otherwise?*)
* nara_acc should have very little missing data - collected at same time as nara_comp, but will be very small proportion due to error
* word_acc & nonword_acc will also have only a very small proportion - collected in same assessment session as nara. Possibly more likely that if word_acc missing then nonword_acc also missing (?), but numbers very small. 
* wold_comp - more likely to have missing data; collected at different clinic session one year earlier 

EJ: For this analysis, it doesn't seem as if there are specific contingencies in missingness (unless we need to incorporate missing nara_comp somehow?), but some variability in likely proportions due to whether the data were collected in same/different clinic visits. I have updated the code to incorporate different proportions.

```{r missing-data}
# specify variables collected in same session, low proportion missingness
same_na_cols <- c("naraAcc", "wordAcc", "nonwordAcc")
same_na_prop <- 0.01

# specify variables collected in separate session, higher proportion missingness
sep_na_cols <- "woldComp"
sep_na_prop <- 0.05

# Create dataframe with NAs
sim_dat_NA <- sim_dat %>% 
  pivot_longer(cols = -c(id,k),names_to = "var", values_to = "value") %>%    # pivot data to long format
  mutate(r = runif(nrow(.)),                                            # simulate a random number (r) from 0 to 1 for each row
         value = ifelse(var %in% same_na_cols & r <= same_na_prop, NA,  # for same session vars, update to NA if r < threshold
                        ifelse(var %in% sep_na_cols & r <= sep_na_prop, NA, value))) %>%  # for separate session vars, update to NA is r < threshold, else same value
  dplyr::select(-r) %>%                                                        # remove random number
  pivot_wider(names_from = var, values_from = value)                    # pivot back to original format
```


# Data check

Inspect properties of the variables.
**Q:** Why overall correlations so high?  Because current groups are ordered ability groups?

```{r summarise-vars}
# Quick summary statistics
describe(sim_dat_NA)

# Overall histograms/correlations
sim_dat_NA %>% 
  select(-k, -id) %>% 
  pairs.panels()

# Distributions/correlations by group
sim_dat_NA %>% 
  select(-id) %>% 
  ggpairs(mapping = aes(color = k))
```

### Missing data

Inspect missingness in the data.

```{r missingness}
vis_miss(sim_dat_NA)
```
(Other tools for visualising relationships in missingness: https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

*Don't need to do anything in advance to deal with this?* 


# Prepare data for Mplus

### Preparing column names

Mplus has an 8-character limit, so may need to rename to avoid duplicates. 

```{r mplus-names}
# Check names
str_sub(names(sim_dat_NA), 1, 8)

# Rename any necessary, print new names
sim_dat_NA <- sim_dat_NA %>% 
  rename(nonwAcc = nonwordAcc)

names(sim_dat_NA)
```

### Missing data

Should use numeric code for missing data, and then specify the missing value code in the mplus analyses???

- tested without reformatting, seems to be fine, but temporarily leaving section here as reminder in case need to approach differently... 

### Reformatting

Variable format problematic for mplus data conversion, change ID to factor.

```{r mplus-formatting}
sim_dat_NA <- sim_dat_NA %>% 
  mutate(id = as.factor(id))
```

### Check that mplus reading data as expected

Extract descriptive statistics as check.
(Could check as step once modelling instead if preferred)

```{r mplus-desc}

if(dir.exists("mod_scripts")==FALSE){dir.create("mod_scripts")} #PT - added this to create a folder in the directory, otherwise have to create manually and the script fails.
#if(dir.exists("mod_scripts/cfa")==FALSE){dir.create("mod_scripts/cfa")} #PT - added this to create a folder in the directory, otherwise have to create manually and the script fails.
#if(dir.exists("mod_scripts/lpa")==FALSE){dir.create("mod_scripts/lpa")} #PT - added this to create a folder in the directory, otherwise have to create manually and the script fails.

# Specify model
m_desc <- mplusObject(
  TITLE = "Data check - Descriptive statistics;",
  ANALYSIS = "type = basic;",
  usevariables = c("naraComp", "naraAcc", "wordAcc", "nonwAcc", "woldComp"),
  rdata = sim_dat_NA)

# Fit model 
m_desc_fit <- mplusModeler(m_desc,
                            dataout = "./mod_scripts/sv_sim.dat",
                            modelout = "./mod_scripts/sv_check.inp",
                            check = TRUE, run = TRUE, hashfilename = TRUE)

# Read mplus output, check that as expected
m_desc_out <- readModels("./mod_scripts/sv_check.out")
m_desc_out$sampstat$univariate.sample.statistics

describe(sim_dat_NA)
```

# Step 1 - Confirmatory Factor Analysis

*Can also run version with tests for non-normality, but probably not an issue in full dataset for this analysis? Might be in poor comprehender LPA though.*
PT - The model can cope with some amount of departures from normality on individual items, it only has the assumption of multivariate normality conditional on class. ITs likely that skew and floor/ceiling effects will be tricky.

### Examining a 2-factor model

```{r cfa-factor-structure}

# ONE-FACTOR MODEL
m_cfa1 <- mplusObject(
  TITLE = "Confirmatory Factor Analysis - Single Factor;",
  ANALYSIS = "estimator = mlr; type = general;",                  # check estimator with PT
  MODEL = "read by naraComp naraAcc wordAcc nonwAcc woldComp;",   # single reading factor
  OUTPUT = "sampstat; TECH1; stdyx; modindices; ",
  PLOT = "TYPE = PLOT3;",
  usevariables = colnames(sim_dat_NA[,!names(sim_dat_NA) %in% c("id","k")]),
  rdata = sim_dat_NA)

m_cfa1_fit <- mplusModeler(m_cfa1,
                           modelout = "./mod_scripts/sv_cfa1.inp",
                           check = TRUE, run = TRUE)

# TWO-FACTOR MODEL
m_cfa2 <- update(m_cfa1,
                 TITLE = ~ "Confirmatory Factor Analysis - Two-Factor;",
                 MODEL = ~ "acc by naraAcc wordAcc nonwAcc; comp by naraComp woldComp;")
m_cfa2_fit <- mplusModeler(m_cfa2,
                           modelout = "./mod_scripts/sv_cfa2.inp",
                           check = TRUE, run = TRUE)

# COMPARE MODELS
cfa_models <- readModels(target = "./mod_scripts", filefilter = "sv_cfa")
SummaryTable(cfa_models, keepCols = c("Title", "Parameters", "LL", "CFI", "TLI", "AIC", "BIC", "RMSEA_Estimate", "RMSEA_pLT05", "SRMR"))
```

**Notes:**  

* CFI & TLI > .95  
* RMSEA <= .05  
* SRMR < .05

### Improving model fit

```{r cfa-inspection}
# Inspect model (by = loadings, with = covariances)
cfa_models$sv_cfa2.out$parameters$unstandardized
cfa_models$sv_cfa2.out$parameters$stdyx.standardized

# Inspect modification indices
cfa_models$sv_cfa2.out$mod_indices
```

* Check for very large standardised residuals
* No guidance on how large MIs should be. Change only one at a time, starting with largest, but must be theoretically meaningful. 

**PT - I usually follow this approach. It usually gets fairly obvious whether the mod indices are making a big difference to refine the model or whether they are masking the problem of that the model is just not suitable for the data. I have found that it typically only needs a few of the very largest (magnitudes bigger than the others) make a huge impact and typically, are able to be tied into a theoretical argument, like items from same instrument are correlated.**


E.g...

```{r cfa-modification}
# TWO-FACTOR MODEL with additional covariance between NARA accuracy and comprehension
m_cfa2b <- update(m_cfa2,
                 TITLE = ~ "Confirmatory Factor Analysis - Two-Factor - modification1 nara covariance;",
                 MODEL = ~. + "naraAcc with naraComp;")

m_cfa2b_fit <- mplusModeler(m_cfa2b,
                           modelout = "./mod_scripts/sv_cfa2b.inp",
                           check = TRUE, run = TRUE)

# COMPARE MODELS
cfa_models <- readModels(target = "./mod_scripts", filefilter = "sv_cfa")
SummaryTable(cfa_models, keepCols = c("Title", "Parameters", "LL", "CFI", "TLI", "AIC", "BIC", "RMSEA_Estimate", "RMSEA_pLT05", "SRMR"))

# Inspect model (by = loadings, with = covariances)
cfa_models$sv_cfa2b.out$parameters$unstandardized
cfa_models$sv_cfa2b.out$parameters$stdy.standardized

# Inspect modification indices
cfa_models$sv_cfa2b.out$mod_indices
```

### CFA figure

*TO-DO: format properly*

```{r cfa-diagram}
semPlotModel(cfa_models$sv_cfa2b.out)
semPaths(semPlotModel(cfa_models$sv_cfa2b.out, mplusStd = "stdyx"), what = "paths", whatLabels = "std", rotation = 2, intercepts = FALSE)
```

Looking at the output of these models, the likelihood ratio test is suggesting that the two factor model is better but the correlation between factors is quite high >0.9, so I'm considering whether they really are distinct factors. We have two ways to go: i) use the two factor model and feed into the latent profile model, but this would need to be converted to a factor mixture model so that we have two factors for comp and acc, but a higher class factor. See here:

```{r path_diag}
library(DiagrammeR)

grViz("
digraph SEM {

graph [layout = neato,
       overlap = false,
       outputorder = edgesfirst]

node [shape = rectangle,fontsize=20]


a [pos = '-5,5!', label = 'woldcomp']
b [pos = '-3,5!', label = 'naracomp']
c [pos = '0,5!', label = 'naraAcc']
d [pos = '2,5!', label = 'wordAcc']
e [pos = '4,5!', label = 'nonwAcc']

f [pos = '-4,7!', label = 'comp', shape = ellipse,fontsize=20]

g [pos = '2,7!', label = 'acc', shape = ellipse,fontsize=20]


h [pos = '-1,9!', label = 'C', shape = ellipse,fontsize=20]


f->a
f->b 

g->c
g->d
g->e

h->f 
h->g

b->c [dir = both]
c->d [dir = both]
d->e [dir = both]
}
")


#d->c [dir = both]
```

I think we should make this decision with the real data, so I have added the code for both scenarios. I tweaked your code as it produces the standard latent profile model like this:

```{r path_diag2}
library(DiagrammeR)

grViz("
digraph SEM {

graph [layout = neato,
       overlap = false,
       outputorder = edgesfirst]

node [shape = rectangle,fontsize=20]


a [pos = '-5,5!', label = 'woldcomp']
b [pos = '-3,5!', label = 'naracomp']
c [pos = '0,5!', label = 'naraAcc']
d [pos = '2,5!', label = 'wordAcc']
e [pos = '4,5!', label = 'nonwAcc']

f [pos = '-1,7!', label = 'C', shape = ellipse,fontsize=20]


f->a
f->b 
f->c
f->d
f->e

b->c [dir = both]
c->d [dir = both]
d->e [dir = both]
}
")


#d->c [dir = both]
```
# Step 2A - Fitting latent profile model (no sub-factors)

### Examining models of different n classes

Using best fitting CFA model, fit LPAs of different number of classes. 

Options reduced to save time for now, but possibly change for actual modelling:
starts - 500 50
lrtbootstrap 50
lrtstarts 50 20 50 20
(taken from Geiser book, could also remove these arguments from below script testing)


```{r lpa-nclass}
# LPA models of different n classes
m_lpa <- lapply(2:6, function(k) {
   body <- update(m_cfa2,
     TITLE = as.formula(sprintf("~ 'Latent profile analysis - %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ . + 'classes = c(%d);'", k)),
     ANALYSIS = ~ "estimator = mlr; type = mixture; stiterations = 50; starts = 250 25;    
                   lrtbootstrap = 200; lrtstarts = 25 5 10 25;",                           
     MODEL = ~ "%OVERALL% 
     naraAcc with naraComp; 
     nonwAcc with	wordAcc;
     nonwAcc with naraAcc;
     wordAcc with naraAcc;",
     OUTPUT = ~ . + "TECH10; TECH11; TECH14;")

   mplusModeler(body, sprintf("mod_scripts/lpa/sv_lpa_%dclass.dat", k), run = TRUE)
 })

# Read in all LPA models
lpa_models_out <- readModels(target = "./mod_scripts/lpa", filefilter = "sv_lpa")
(lpa_mod_summaries <- mixtureSummaryTable(lpa_models_out))
```

```{r lpa-scree}
lpa_mod_summaries %>%  
  select(Classes, AIC, BIC, aBIC) %>% 
  pivot_longer(AIC:aBIC, names_to = "statistic", values_to = "value") %>% 
  ggplot(aes(x = as.factor(Classes), y = value)) + 
  geom_point() + 
  geom_line(group = 1) + 
  xlab("n classes") + 
  theme_bw() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_grid(statistic ~., scales = "free")
```

4 classes best here? PT - I don't think this shows good distinction between models, so we might have to adjust the model further.

Why BLRT all 0? PT - "I would go by BIC in these cases. The problem of BIC not showing a minimum can be solved by allowing within-class covariance between some pairs of variables. Sometimes adding a single factor to soak up some such covariance can give an indication of which pairs of items are the culprits." http://www.statmodel.com/discussion/messages/13/4529.html?1249068377.

Other considerations - entropy to approach 1 (<= 0.6 considered poor; Asparouhov & Muthen, 2014); classes meaningful and not too small.


# Step 2B - Fitting Factor mixture model (sub-factors for comprehension and accuracy)

*Aim:* to fit models of different n classes, at range of different model specifications.

Indicator variances can always differ from each other within a class, but different model specifications vary in whether they are separately estimated between classes. Labels for different specifications taken from Pastor (2007) and Masyn (2013).

### Base model

Set up initial model, based on CFA above. 

Options reduced to save time for now, but possibly change for actual modelling:
starts - 500 50
lrtbootstrap 50
lrtstarts 50 20 50 20
(taken from Geiser book)

```{r fmm-base}
# Specify final selected model
select_cfa_spec <- m_cfa2b$MODEL

# Create mixture model specification
model_spec <- paste0("%OVERALL% \n", select_cfa_spec)
#model_spec <- "%OVERALL% \n acc by naraAcc wordAcc nonwAcc; \n comp by naraComp woldComp;"  # tried to make sure NARA covariance wasn't problem for A/C

# Create base model
m_fmm_base <- mplusObject(
  TITLE = "Factor mixture model;",
  ANALYSIS = "estimator = mlr; type = mixture; starts = 100 10;",
  VARIABLE = "classes = c(1);",
  MODEL = model_spec,
  OUTPUT = "sampstat; stdyx; TECH1; TECH8; TECH11; TECH14;",
  PLOT = "TYPE = PLOT3;",
  usevariables = colnames(sim_dat_NA[,!names(sim_dat_NA) %in% c("id","k")]),
  rdata = sim_dat_NA)
```


### Model A: Class-invariant, diagonal

Indicator variances differ from each other within a class, but are constrained to be equal across classes. No covariances are estimated. 

```{r fmm-a}
m_fmm_a <- lapply(2:5, function(k) {
   body <- update(m_fmm_base,
     TITLE = as.formula(sprintf("~ 'FMM Model A: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'classes = c(%d);'", k)),
     MODEL = ~ . + "acc with comp@0;")

   mplusModeler(body, sprintf("mod_scripts/sv_fmm_a_%dclass.dat", k), run = TRUE)
 })


# CHECK WORKING
fmm_a_out <- readModels(target = "./mod_scripts", filefilter = "sv_fmm_a")
(fmm_a_summaries <- mixtureSummaryTable(fmm_a_out))
```

**I THINK THIS IS WRONG - STILL SEEMS TO BE ESTIMATING COVARIANCES?** (i.e., same as Model B below)
** Now correct as the covariance between factors needs to be set specifically to zero. Model removes this constraint.
### Model B: Class-invariant, unrestricted

Indicator variances differ from each other within a class and can covary, but variances and covariances constrained to be equal across classes.

```{r fmm-b}
m_fmm_b <- lapply(2:5, function(k) {
   body <- update(m_fmm_base,
     TITLE = as.formula(sprintf("~ 'FMM Model B: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'classes = c(%d);'", k)),
     MODEL = ~ . + "acc with comp;")

   mplusModeler(body, sprintf("mod_scripts/sv_fmm_b_%dclass.dat", k), run = TRUE)
 })


# CHECK WORKING
fmm_b_out <- readModels(target = "./mod_scripts", filefilter = "sv_fmm_b")
(fmm_b_summaries <- mixtureSummaryTable(fmm_b_out))
```

### Model C: Class-varying, diagonal 

Indicator variances are estimated separately for each class. No covariances are estimated.

```{r fmm-c}
m_fmm_c <- lapply(3, function(k) {
  
  # Create class-level specifications
  class_spec <- "%c#1% \n acc; \n comp;"
  for (i in 2:k){
    class_spec <- paste0(class_spec, "\n %c#", i, "% \n acc; \n comp;")
  }
  
  # Update model
  body<-update(m_fmm_base,
              MODEL = ~ . + "acc with comp@0;" )
  
  body <- update(body,
     TITLE = as.formula(sprintf("~ 'FMM Model C: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))

   mplusModeler(body, sprintf("mod_scripts/sv_fmm_c_%dclass.dat", k), run = TRUE)
 })


# CHECK WORKING
lpa_c_out <- readModels(target = "./mod_scripts", filefilter = "sv_lpa_c")
(lpa_c_summaries <- mixtureSummaryTable(lpa_c_out))
```

**I THINK THIS IS WRONG - STILL SEEMS TO BE ESTIMATING COVARIANCES?** (i.e., same as Model D below)
** now corrected as the covariance in C is set to zero. The variances are allowed to differ between classes. I had to implement this using a double update as I could figure out how to add in top level constraint without affecting the class looped bit.

### Model D 

Indicator variances are estimated separately for each class, but covariances are constrained to be equal across classes.

```{r fmm-d}
m_fmm_d <- lapply(2:5, function(k) {
  
  # Create class-level specifications
  class_spec <- "%c#1% \n acc; \n comp;"
  for (i in 2:k){
    class_spec <- paste0(class_spec, "\n %c#", i, "% \n acc; \n comp;")
  }
  
  # Update model
   body <- update(m_fmm_b[[1]],
     TITLE = as.formula(sprintf("~ 'fmm Model D: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))

   mplusModeler(body, sprintf("mod_scripts/sv_fmm_d_%dclass.dat", k), run = TRUE)
 })


# CHECK WORKING
fmm_d_out <- readModels(target = "./mod_scripts", filefilter = "sv_fmm_d")
(fmm_d_summaries <- mixtureSummaryTable(fmm_d_out))
```

### Model E: Class-varying, unrestricted

All variances and covariances can vary between clusters. 

```{r fmm-e}
m_fmm_e <- lapply(2:5, function(k) {
  
  # Create class-level specifications
  class_spec <- "%c#1% \n acc; \n comp; \n acc with comp;"
  for (i in 2:k){
    class_spec <- paste0(class_spec, "\n %c#", i, "% \n acc; \n comp; \n acc with comp;")
  }
  
  # Update model
   body <- update(m_fmm_b[[1]],
     TITLE = as.formula(sprintf("~ 'FMM Model E: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))

   mplusModeler(body, sprintf("mod_scripts/sv_fmm_e_%dclass.dat", k), run = TRUE)
 })


# CHECK WORKING
fmm_e_out <- readModels(target = "./mod_scripts", filefilter = "sv_fmm_e")
(fmm_e_summaries <- mixtureSummaryTable(fmm_e_out))
```


# Inspect all models

```{r fmm-all-mods}
fmm_all_out <- readModels(target = "./mod_scripts", filefilter = "sv_fmm_")
(fmm_all_summaries <- mixtureSummaryTable(fmm_all_out))

```

```{r lpa-scree}
fmm_all_summaries %>%  
  select(Classes, AIC, BIC, aBIC) %>% 
  pivot_longer(AIC:aBIC, names_to = "statistic", values_to = "value") %>% 
  ggplot(aes(x = as.factor(Classes), y = value)) + 
  geom_point() + 
  geom_line(group = 1) + 
  xlab("n classes") + 
  theme_bw() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_grid(statistic ~., scales = "free")
```


################################### to here ##############################################

(below are previous bits of code, some still to implement above)






```{r lpa-nclass}
# Basic LPA
base_lpa <- lapply(1, function(k) {
   body <- update(select_cfa,
     TITLE = as.formula(sprintf("~ 'class-%d;'", k)),
     VARIABLE = as.formula(sprintf("~ . + 'classes = c(%d);'", k)),
     ANALYSIS = ~ "estimator = mlr; type = mixture; stiterations = 50; starts = 100 10;    
                   lrtbootstrap = 50; lrtstarts = 25 5 10 25;",
     MODEL = ~ "%OVERALL%",
     OUTPUT = ~ . + "TECH10; TECH11; TECH14;")

   mplusModeler(body, "mod_scripts/lpa_base.dat", run = TRUE)
 })


# LPA models of different n classes
m_lpa_a <- lapply(2:3, function(k) {
   body <- update(select_cfa,
     TITLE = as.formula(sprintf("~ 'Latent profile analysis: Model A - %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ . + 'classes = c(%d);'", k)),
     MODEL = ~ . + eval(mget(select_cfa_spec)),
     OUTPUT = ~ . + "TECH10; TECH11; TECH14;")

   mplusModeler(body, sprintf("mod_scripts/sv_lpa_a_%dclass.dat", k), run = TRUE)
 })

# Read in all LPA models
lpa_a_out <- readModels(target = "./mod_scripts", filefilter = "sv_lpa_a")
(lpa_a_summaries <- mixtureSummaryTable(lpa_a_out))
```


p200 Mplus manual (example 7.17 mixture CFA modelling)
not sure on the acc*1 versus just acc part - makes a difference, still to look into

```{r}
m_trial <- mplusObject(
  TITLE = "Factor mixture model;",
  ANALYSIS = "estimator = mlr; type = mixture;",
  VARIABLE = "classes = c(3);",
  MODEL = "
  %OVERALL% 
  acc by naraAcc wordAcc nonwAcc;  !factor for reading accuracy
  comp by naraComp woldComp;       !factor for comprehension
  naraAcc with naraComp;           !example modification guided by initial CFA
  acc with comp;                   !allows covariances to be estimated within clusters
  
  %c#1%                            
  acc;                             !allows for variances to differ between clusters
  comp;
  acc with comp;                   !allows covariances to differ across clusters
  
  %c#2% 
  acc;
  comp;
  acc with comp;                   
  
  %c#3% 
  acc;
  comp;
  acc with comp;",                  
  OUTPUT = "sampstat; TECH1; TECH8; stdyx;",
  PLOT = "TYPE = PLOT3;",
  usevariables = colnames(sim_dat_NA[,!names(sim_dat_NA) %in% c("id","k")]),
  rdata = sim_dat_NA,
  SAVEDATA = "FILE IS class_assignment_trial.dat; SAVE = cprobabilities;")

trial_fit <- mplusModeler(m_trial,
                           modelout = "./mod_scripts/testing_mixturecfa.inp",
                           check = TRUE, run = TRUE)



# COMPARE MODELS
trial_fitted <- readModels(target = "./mod_scripts", filefilter = "testing_mixture")
SummaryTable(trial_fitted)

plotMixtureDensities(lp, variables = c("Acc", "Comp"))
plotMixtures(trial_fitted, variables = c("Acc", "Comp"))

semPaths(semPlotModel(trial_fitted, mplusStd = "stdyx"), what = "paths", whatLabels = "std", rotation = 2, intercepts = FALSE)
semPaths(trial_fitted, what = "paths", whatLabels = "std", intercepts = FALSE)
```


```{r lpa-nclass2}
# LPA models of different n classes
m_lpa_e <- lapply(2:6, function(k) {
   body <- update(m_lpa_a,
     TITLE = as.formula(sprintf("~ 'Latent profile analysis: Model E - %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ . + 'classes = c(%d);'", k)),
     ANALYSIS = ~ "estimator = mlr; type = mixture; stiterations = 50; starts = 100 10;    
                   lrtbootstrap = 50; lrtstarts = 25 5 10 25;",                           
     MODEL = ~ "%OVERALL%",
     OUTPUT = ~ . + "TECH10; TECH11; TECH14;")

   mplusModeler(body, sprintf("mod_scripts/sv_lpa_a_%dclass.dat", k), run = TRUE)
 })

# Read in all LPA models
#lpa_a_out <- readModels(target = "./mod_scripts", filefilter = "sv_lpa_a")
#(lpa_a_summaries <- mixtureSummaryTable(lpa_a_out))
```



```{r lpa-scree}
lpa_mod_summaries %>%  
  select(Classes, AIC, BIC, aBIC) %>% 
  pivot_longer(AIC:aBIC, names_to = "statistic", values_to = "value") %>% 
  ggplot(aes(x = as.factor(Classes), y = value)) + 
  geom_point() + 
  geom_line(group = 1) + 
  xlab("n classes") + 
  theme_bw() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_grid(statistic ~., scales = "free")
```

4 classes best here?
Why BLRT all 0? 

Other considerations - entropy to approach 1 (<= 0.6 considered poor; Asparouhov & Muthen, 2014); classes meaningful and not too small.

**TO-DO: What about different variance/covariance structures??**

### Inspecting resultant model

(Focused on three here to test recovery of simulated groups, but might not have selected??)
Note - may be able to use auxiliary argument to keep participant IDs, but may need to amend data input above. 

```{r lpa-classes}

# Select model specification for selected model
m_lpa_select <- m_lpa[[2]]

# Re-fit to extract data
m_lpa_extract <- update(m_lpa_select,
#                  VARIABLE = ~ . + "auxiliary = id k;",
                  SAVEDATA = ~ "FILE IS class_assignment_3class.dat; SAVE = cprobabilities;")
m_lpa_select_fit <- mplusModeler(m_lpa_extract, modelout = "./mod_scripts/sv_lpa_select.inp", run = TRUE)

m_lpa_select_out <- readModels("./mod_scripts/sv_lpa_select.out")

# Inspect classes
plotMixtureDensities(m_lpa_select_out)
plotMixtures(m_lpa_select_out)

# Model parameters
m_lpa_select_out$parameters

# FIGURE?? have not succeeded here yet
# semPaths(semPlotModel(final_lpa, mplusStd = "stdyx"), what = "mod", whatLabels = "est", intercepts = FALSE)
```
Extract data 

```{r lpa-assignement}
class_data <- m_lpa_select_out$savedata
```

To test from simulation (matched up classes manually for max fit)

```{r sim-class-check}

# Matched up simulated classes and resultant model
sim_mod <- bind_cols(sim_dat_NA, class_data) %>% 
  mutate(comp_check = as.logical(round(naraComp,3) == NARACOMP),   # check data correctly aligned
         acc_check = as.logical(round(nonwAcc,3) == NONWACC),
         c_to_k = ifelse(C == 3, 1, 
                         ifelse(C == 2, 3, 2)),
         class_check = as.logical(k == c_to_k))

# class matching (informs above)
sim_mod %>% 
  group_by(k, C) %>% 
  count()


# Proportion classes correctly recovered
mean(sim_mod$class_check)
```





######################################## Paul's code ####################################



# Fitting simple LPA model in Mplus (manually specifying the mplus model)

```{r mplus-prep}


#prepare data, converting R data.frame to Mplus readable file.
sim_dat_NA2<-sim_dat_NA

names(sim_dat_NA2)<-c("k", "id", "naraComp", "naraAcc", "wordAcc", "nonwordAcc", "woldComp")

#prepare the model script file. This can be done using a .txt file then using 


pathmodel3 <- mplusObject(
TITLE= "LCA with continuous latent class indicators;",
VARIABLE = "CLASSES = c (3);",
ANALYSIS = "
TYPE = MIXTURE; 
STARTS = 0;",
MODEL = "
%OVERALL% 
%c#1% 
[naraComp-woldComp]; 
%c#2% 
[naraComp-woldComp];
%c#3% 
[naraComp-woldComp];",
OUTPUT = "TECH1 TECH8;",
usevariables = c("naraComp", "naraAcc", "wordAcc", "nonwordAcc", "woldComp"),
rdata = sim_dat_NA2[,!names(sim_dat_NA2) %in% c("id","k")])

pathmodel2 <- mplusObject(
TITLE= "LCA with continuous latent class indicators (2);",
VARIABLE = "CLASSES = c (2);",
ANALYSIS = "
TYPE = MIXTURE; 
STARTS = 0;",
MODEL = "
%OVERALL% 
%c#1% 
[naraComp-woldComp]; 
%c#2% 
[naraComp-woldComp];",
OUTPUT = "TECH1 TECH8;",
usevariables = c("naraComp", "naraAcc", "wordAcc", "nonwordAcc", "woldComp"),
rdata = sim_dat_NA2[,!names(sim_dat_NA2) %in% c("id","k")])

fit2 <- mplusModeler(pathmodel2, modelout = "model_LPA2.inp", run = 1L,check=TRUE,varwarnings=TRUE)
fit3 <- mplusModeler(pathmodel3, modelout = "model_LPA3.inp", run = 1L,check=TRUE,varwarnings=TRUE)


screenreg(fit2, single.row=TRUE)
screenreg(fit3, single.row=TRUE)

```


```{r multi_template}

m.cfa <- mplusObject(
    TITLE = "Confirmatory Factor Analysis",
    ANALYSIS = "estimator = mlr;",
    MODEL = "factor BY naraComp-woldComp;",
    OUTPUT = "TECH1; TECH8;",
    PLOT = "TYPE = PLOT3;",
    usevariables = colnames(sim_dat_NA2[,!names(sim_dat_NA2) %in% c("id","k")]),
    rdata = sim_dat_NA2[,!names(sim_dat_NA2) %in% c("id","k")])

m.cfa.fit <- mplusModeler(m.cfa, "cfa_example.dat", run = TRUE)

m.lca <- lapply(2:5, function(k) {
   body <- update(m.cfa,
     TITLE = as.formula(sprintf("~ 'class-%d;'", k)),
     VARIABLE = as.formula(sprintf("~ . + 'classes = c(%d);'", k)),
     ANALYSIS = ~ . + "type = mixture; starts = 250 25;",
     MODEL = ~ "%OVERALL%",
     OUTPUT = ~ . + "TECH14;")

   mplusModeler(body, sprintf("lpa_%d_example.dat", k), run = TRUE)
 })

LPAModels <- readModels("/Volumes/PSYHOME/PSYRES/pthompson/DVMB/ESRC_EJ_WP1/mplus_test_scripts")

```

